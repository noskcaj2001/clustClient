{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color= 'green'>   [01] Código para correlação das variáveis númericas </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset background style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Calculate the correlation matrix excluding the 'CustomerID' column\n",
    "corr = customer_data_cleaned.drop(columns=['CustomerID']).corr()\n",
    "\n",
    "# Define a custom colormap\n",
    "colors = ['#ff6200', '#ffcaa8', 'white', '#ffcaa8', '#ff6200']\n",
    "my_cmap = LinearSegmentedColormap.from_list('custom_map', colors, N=256)\n",
    "\n",
    "# Create a mask to only show the lower triangle of the matrix (since it's mirrored around its \n",
    "# top-left to bottom-right diagonal)\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, mask=mask, cmap=my_cmap, annot=True, center=0, fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estas correlações elevadas indicam que estas variáveis ​​se movem muito próximas umas das outras, implicando um certo grau de multicolinearidade.\n",
    "\n",
    "### principais impactos:\n",
    "\n",
    "`Métrica de Distância:` Muitos algoritmos de clusterização, como K-means e hierárquico, dependem de métricas de distância (por exemplo, a distância euclidiana) para determinar a similaridade entre pontos de dados. A multicolinearidade pode distorcer essas distâncias, já que variáveis altamente correlacionadas podem dominar a métrica de distância, influenciando desproporcionalmente a formação dos clusters.\n",
    "\n",
    "\n",
    "`Clusters Redundantes:` Variáveis altamente correlacionadas podem fornecer informações redundantes, resultando em clusters que refletem mais essa redundância do que diferenças reais entre os grupos de dados\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> [02] Pipeline </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Suponha que `df_clean` seja seu DataFrame\n",
    "numeric_features = df_clean.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Defina os passos do pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputação para valores numéricos\n",
    "    ('scaler', StandardScaler())  # Padronização para valores numéricos\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputação para valores categóricos usando a moda\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Codificação one-hot para variáveis categóricas\n",
    "])\n",
    "\n",
    "# Combinar etapas de pré-processamento para todos os tipos de features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),  # numeric_features são suas variáveis numéricas\n",
    "        ('cat', categorical_transformer, categorical_features)  # categorical_features são suas variáveis categóricas\n",
    "    ])\n",
    "\n",
    "# Definir o pipeline completo\n",
    "pipeline_stand = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "])\n",
    "\n",
    "# Agora você pode usar este pipeline para ajustar e transformar seus dados\n",
    "pipeline_stand.fit(df_clean)\n",
    "processed_data_stand = pipeline_stand.transform(df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Suponha que `df_clean` seja seu DataFrame\n",
    "numeric_features = df_clean.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Defina os passos do pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputação para valores numéricos\n",
    "    ('normalizer', Normalizer())  # Normalização para valores numéricos\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputação para valores categóricos usando a moda\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Codificação one-hot para variáveis categóricas\n",
    "])\n",
    "\n",
    "# Combinar etapas de pré-processamento para todos os tipos de features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),  # numeric_features são suas variáveis numéricas\n",
    "        ('cat', categorical_transformer, categorical_features)  # categorical_features são suas variáveis categóricas\n",
    "    ])\n",
    "\n",
    "# Definir o pipeline completo\n",
    "pipeline_norm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "])\n",
    "\n",
    "# Agora você pode usar este pipeline para ajustar e transformar seus dados\n",
    "pipeline_norm.fit(df_clean)\n",
    "processed_data_norm = pipeline_norm.transform(df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Min-max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Suponha que `df_clean` seja seu DataFrame\n",
    "numeric_features = df_clean.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Defina os passos do pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputação para valores numéricos\n",
    "    ('normalizer', Normalizer())  # Normalização para valores numéricos\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputação para valores categóricos usando a moda\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Codificação one-hot para variáveis categóricas\n",
    "])\n",
    "\n",
    "# Combinar etapas de pré-processamento para todos os tipos de features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),  # numeric_features são suas variáveis numéricas\n",
    "        ('cat', categorical_transformer, categorical_features)  # categorical_features são suas variáveis categóricas\n",
    "    ])\n",
    "\n",
    "# Definir o pipeline completo\n",
    "pipeline_MinMax = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "])\n",
    "\n",
    "# Agora você pode usar este pipeline para ajustar e transformar seus dados\n",
    "pipeline_MinMax.fit(df_clean)\n",
    "processed_data_MinMax = pipeline_MinMax.transform(df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> [03] PCA </font>\n",
    "\n",
    "`Metodologia`\n",
    "\n",
    "Aplicarei o PCA em todos os componentes disponíveis e traçarei a variação cumulativa explicada por eles. Este processo me permitirá visualizar quanta variação cada componente principal adicional pode explicar, ajudando-me assim a identificar o número ideal de componentes a serem retidos para a análise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting CustomerID as the index column\n",
    "customer_data_scaled.set_index('CustomerID', inplace=True)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA().fit(customer_data_scaled)\n",
    "\n",
    "# Calculate the Cumulative Sum of the Explained Variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Set the optimal k value (based on our analysis, we can choose 6)\n",
    "optimal_k = 6\n",
    "\n",
    "# Set seaborn plot style\n",
    "sns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')\n",
    "\n",
    "# Plot the cumulative explained variance against the number of components\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Bar chart for the explained variance of each component\n",
    "barplot = sns.barplot(x=list(range(1, len(cumulative_explained_variance) + 1)),\n",
    "                      y=explained_variance_ratio,\n",
    "                      color='#fcc36d',\n",
    "                      alpha=0.8)\n",
    "\n",
    "# Line plot for the cumulative explained variance\n",
    "lineplot, = plt.plot(range(0, len(cumulative_explained_variance)), cumulative_explained_variance,\n",
    "                     marker='o', linestyle='--', color='#ff6200', linewidth=2)\n",
    "\n",
    "# Plot optimal k value line\n",
    "optimal_k_line = plt.axvline(optimal_k - 1, color='red', linestyle='--', label=f'Optimal k value = {optimal_k}') \n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Explained Variance', fontsize=14)\n",
    "plt.title('Cumulative Variance vs. Number of Components', fontsize=18)\n",
    "\n",
    "# Customize ticks and legend\n",
    "plt.xticks(range(0, len(cumulative_explained_variance)))\n",
    "plt.legend(handles=[barplot.patches[0], lineplot, optimal_k_line],\n",
    "           labels=['Explained Variance of Each Component', 'Cumulative Explained Variance', f'Optimal k value = {optimal_k}'],\n",
    "           loc=(0.62, 0.1),\n",
    "           frameon=True,\n",
    "           framealpha=1.0,  \n",
    "           edgecolor='#ff6200')  \n",
    "\n",
    "# Display the variance values for both graphs on the plots\n",
    "x_offset = -0.3\n",
    "y_offset = 0.01\n",
    "for i, (ev_ratio, cum_ev_ratio) in enumerate(zip(explained_variance_ratio, cumulative_explained_variance)):\n",
    "    plt.text(i, ev_ratio, f\"{ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "    if i > 0:\n",
    "        plt.text(i + x_offset, cum_ev_ratio + y_offset, f\"{cum_ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.grid(axis='both')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a tabela de PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PCA object with 6 components\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "# Fitting and transforming the original data to the new PCA dataframe\n",
    "customer_data_pca = pca.fit_transform(customer_data_scaled)\n",
    "\n",
    "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
    "customer_data_pca = pd.DataFrame(customer_data_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
    "\n",
    "# Adding the CustomerID index back to the new PCA dataframe\n",
    "customer_data_pca.index = customer_data_scaled.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando as variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to highlight the top 3 absolute values in each column of a dataframe\n",
    "def highlight_top3(column):\n",
    "    top3 = column.abs().nlargest(3).index\n",
    "    return ['background-color:  #ffeacc' if i in top3 else '' for i in column.index]\n",
    "\n",
    "# Create the PCA component DataFrame and apply the highlighting function\n",
    "pc_df = pd.DataFrame(pca.components_.T, columns=['PC{}'.format(i+1) for i in range(pca.n_components_)],  \n",
    "                     index=customer_data_scaled.columns)\n",
    "\n",
    "pc_df.style.apply(highlight_top3, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando os principais compopnentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the color scheme for the clusters (RGB order)\n",
    "colors = ['#e8000b', '#1ac938', '#023eff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate data frames for each cluster\n",
    "cluster_0 = customer_data_pca[customer_data_pca['cluster'] == 0]\n",
    "cluster_1 = customer_data_pca[customer_data_pca['cluster'] == 1]\n",
    "cluster_2 = customer_data_pca[customer_data_pca['cluster'] == 2]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points for each cluster separately and specify the color\n",
    "fig.add_trace(go.Scatter3d(x=cluster_0['PC1'], y=cluster_0['PC2'], z=cluster_0['PC3'], \n",
    "                           mode='markers', marker=dict(color=colors[0], size=5, opacity=0.4), name='Cluster 0'))\n",
    "fig.add_trace(go.Scatter3d(x=cluster_1['PC1'], y=cluster_1['PC2'], z=cluster_1['PC3'], \n",
    "                           mode='markers', marker=dict(color=colors[1], size=5, opacity=0.4), name='Cluster 1'))\n",
    "fig.add_trace(go.Scatter3d(x=cluster_2['PC1'], y=cluster_2['PC2'], z=cluster_2['PC3'], \n",
    "                           mode='markers', marker=dict(color=colors[2], size=5, opacity=0.4), name='Cluster 2'))\n",
    "\n",
    "# Set the title and layout details\n",
    "fig.update_layout(\n",
    "    title=dict(text='3D Visualization of Customer Clusters in PCA Space', x=0.5),\n",
    "    scene=dict(\n",
    "        xaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC1'),\n",
    "        yaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC2'),\n",
    "        zaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC3'),\n",
    "    ),\n",
    "    width=900,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribuição dos Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of customers in each cluster\n",
    "cluster_percentage = (customer_data_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
    "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
    "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n",
    "\n",
    "# Adding percentages on the bars\n",
    "for index, value in enumerate(cluster_percentage['Percentage']):\n",
    "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
    "\n",
    "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
    "plt.xticks(ticks=np.arange(0, 50, 5))\n",
    "plt.xlabel('Percentage (%)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outras Métricas de avaliação de Clusters :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Métrica                          | Descrição                                                                                                                                                       |\n",
    "|----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Índice de Silhueta (Silhouette Score)   | O índice de silhueta mede a separação dos clusters. Um valor próximo de 1 indica clusters bem separados, enquanto valores próximos de 0 ou negativos indicam sobreposição entre clusters.                                                   |\n",
    "| Calinski-Harabasz Score          | O índice Calinski-Harabasz é uma medida da dispersão entre os clusters em relação à dispersão dentro dos clusters. Valores mais altos indicam clusters mais bem definidos. Interpretação: Quanto maior o valor, melhor a definição dos clusters.                                     |\n",
    "| Índice Davies-Bouldin (Davies Bouldin Score) | O índice Davies-Bouldin avalia a similaridade média entre cada cluster e seu cluster mais semelhante. Valores mais baixos indicam uma melhor separação entre os clusters. Interpretação: Quanto mais próximo de 0, melhor a separação entre os clusters.                                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Métrica                                        | Fórmula                                                                                                                                                       |\n",
    "|------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Índice de Silhueta (Silhouette Score)          | \\( s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}} \\), onde \\( a(i) \\) é a distância média do ponto \\( i \\) para todos os outros pontos no mesmo cluster e \\( b(i) \\) é a menor distância média do ponto \\( i \\) para todos os pontos em um cluster diferente. |\n",
    "| Calinski-Harabasz Score                        | \\( CH = \\frac{SS_{\\text{between}}}{SS_{\\text{within}}} \\times \\frac{N - K}{K - 1} \\), onde \\( SS_{\\text{between}} \\) é a soma dos quadrados das distâncias entre a média de cada cluster e a média global dos dados, e \\( SS_{\\text{within}} \\) é a soma dos quadrados das distâncias de cada ponto ao centroide do seu próprio cluster. |\n",
    "| Índice Davies-Bouldin (Davies Bouldin Score)   | \\( DB = \\frac{1}{K} \\sum_{i=1}^{K} \\max_{i \\neq j} \\left( \\frac{a(i) + a(j)}{R_{ij}} \\right) \\), onde \\( K \\) é o número de clusters, \\( a(i) \\) é a similaridade intra-cluster do cluster \\( i \\), definida como a média das distâncias de todos os pontos do cluster ao centroide do cluster \\( i \\), e \\( R_{ij} \\) é a similaridade entre os clusters \\( i \\) e \\( j \\).           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of customers\n",
    "num_observations = len(customer_data_pca)\n",
    "\n",
    "# Separate the features and the cluster labels\n",
    "X = customer_data_pca.drop('cluster', axis=1)\n",
    "clusters = customer_data_pca['cluster']\n",
    "\n",
    "# Compute the metrics\n",
    "sil_score = silhouette_score(X, clusters)\n",
    "calinski_score = calinski_harabasz_score(X, clusters)\n",
    "davies_score = davies_bouldin_score(X, clusters)\n",
    "\n",
    "# Create a table to display the metrics and the number of observations\n",
    "table_data = [\n",
    "    [\"Number of Observations\", num_observations],\n",
    "    [\"Silhouette Score\", sil_score],\n",
    "    [\"Calinski Harabasz Score\", calinski_score],\n",
    "    [\"Davies Bouldin Score\", davies_score]\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#e8000b', '#1ac938', '#023eff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 'CustomerID' column as index and assigning it to a new dataframe\n",
    "# dfdf = customer_data_cleaned.set_index('CustomerID')\n",
    "\n",
    "# Standardize the data (excluding the cluster_label column)\n",
    "scaler = StandardScaler()\n",
    "dfdf_standardized = scaler.fit_transform(dfdf.drop(columns=['cluster_label'], axis=1))\n",
    "\n",
    "# Create a new dataframe with standardized values and add the cluster_label column back\n",
    "dfdf_standardized = pd.DataFrame(dfdf_standardized, columns=dfdf.columns[:-1], index=dfdf.index)\n",
    "dfdf_standardized['cluster_label'] = dfdf['cluster_label']\n",
    "\n",
    "# Calculate the centroids of each cluster_label\n",
    "cluster_label_centroids = dfdf_standardized.groupby('cluster_label').mean()\n",
    "\n",
    "# Function to create a radar chart\n",
    "def create_radar_chart(ax, angles, data, color, cluster_label):\n",
    "    # Plot the data and fill the area\n",
    "    ax.fill(angles, data, color=color, alpha=0.4)\n",
    "    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')\n",
    "    \n",
    "    # Add a title\n",
    "    ax.set_title(f'cluster_label {cluster_label}', size=20, color=color, y=1.1)\n",
    "\n",
    "# Set data\n",
    "labels=np.array(cluster_label_centroids.columns)\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Compute angle of each axis\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "# The plot is circular, so we need to \"complete the loop\" and append the start to the end\n",
    "labels = np.concatenate((labels, [labels[0]]))\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the figure\n",
    "fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True), nrows=1, ncols=3)\n",
    "\n",
    "# Create radar chart for each cluster_label\n",
    "for i, color in enumerate(colors):\n",
    "    data = cluster_label_centroids.loc[i].tolist()\n",
    "    data += data[:1]  # Complete the loop\n",
    "    create_radar_chart(ax[i], angles, data, color, i)\n",
    "\n",
    "# Add input data\n",
    "ax[0].set_xticks(angles[:-1])\n",
    "ax[0].set_xticklabels(labels[:-1])\n",
    "\n",
    "ax[1].set_xticks(angles[:-1])\n",
    "ax[1].set_xticklabels(labels[:-1])\n",
    "\n",
    "ax[2].set_xticks(angles[:-1])\n",
    "ax[2].set_xticklabels(labels[:-1])\n",
    "\n",
    "# Add a grid\n",
    "ax[0].grid(color='grey', linewidth=0.5)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
